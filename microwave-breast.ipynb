{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom matplotlib.pyplot import plot\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with open('../input/micro-breast/test_data.pickle', 'rb') as fptr:\n     test_data = pickle.load(fptr)\nwith open('../input/micro-breast/test_labels.pickle', 'rb') as fptr:\n     test_labels = pickle.load(fptr)\nwith open('../input/micro-breast/test_md.pickle', 'rb') as fptr:\n     test_md = pickle.load(fptr)\nwith open('../input/micro-breast/train_data.pickle', 'rb') as fptr:\n     train_data = pickle.load(fptr)\nwith open('../input/micro-breast/train_labels.pickle', 'rb') as fptr:\n     train_labels = pickle.load(fptr)\nwith open('../input/micro-breast/train_md.pickle', 'rb') as fptr:\n     train_md = pickle.load(fptr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_samples(data):\n    \"\"\"Normalizes each sample in data to have unity maximum\n    Parameters\n    ----------\n    data : array_like\n        3D array of the features for each sample (assumes 2D features)\n    Returns\n    -------\n    normalized_data : array_like\n        Array of the features for each sample, normalized so that the\n        max value is unity for each sample\n    \"\"\"\n\n    # Assert that data must be 3D\n    assert len(np.shape(data)) == 3, 'Error: data must have 3 dim'\n\n    normalized_data = np.ones_like(data)  # Init array to return\n\n    # For each sample\n    for sample_idx in range(np.size(data, axis=0)):\n\n        # Normalize to have max of unity\n        normalized_data[sample_idx, :, :] = (data[sample_idx, :, :] /\n                                             np.max(data[sample_idx, :, :]))\n\n    return normalized_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert each train sample from the frequency-domain to the time-domain\nfor sample_idx in range(np.size(train_data, axis=0)):\n    train_data[sample_idx, :, :] = np.fft.ifft(train_data[sample_idx, :, :],\n                                               axis=0)\n# Convert each test sample from the frequency-domain to the time-domain\nfor sample_idx in range(np.size(test_data, axis=0)):\n    test_data[sample_idx, :, :] = np.fft.ifft(test_data[sample_idx, :, :],\n                                              axis=0)\n\n# Take the abs-value of ecah sample, and apply the time-domain window\ntrain_data = np.abs(train_data[:, 5:40, :])\ntest_data = np.abs(test_data[:, 5:40, :])\n\n# Normalize each sample to have a maximum of unity\ntrain_data = normalize_samples(train_data)\ntest_data = normalize_samples(test_data)\ntrain_data = np.reshape(train_data, [np.size(train_data, axis=0), 35 * 72])\ntest_data = np.reshape(test_data, [np.size(test_data, axis=0), 35 * 72])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \n        # Define the logistic regression classifier\nlogreg = LogisticRegression()\n\n        # Fit the classifier\nlogreg.fit(train_data, train_labels)\n\nac = accuracy_score(test_labels,logreg.predict(test_data))\n#86%\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(test_labels,logreg.predict(test_data))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nsensitivity=cm[0,0]/ (cm[0,0]+ cm[1,0])\nprint(\"Sensitivity is\",sensitivity)\nspecificity=cm[1,1]/ (cm[0,1]+cm[1,1])\nprint(\"Specificity is\", specificity)\nroc_auc_score(test_labels, logreg.predict(test_data))\nlogreg.get_params()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt= tree.DecisionTreeClassifier(max_depth=15, max_leaf_nodes=100)\n\n        # Fit the classifier\ndt.fit(train_data, train_labels)\n\nac = accuracy_score(test_labels,dt.predict(test_data))\n#82+-4%\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(test_labels,dt.predict(test_data))\nsns.heatmap(cm,annot=True,fmt=\"d\")\nsensitivity=cm[0,0]/ (cm[0,0]+ cm[1,0])\nprint(\"Sensitivity is\",sensitivity)\nspecificity=cm[1,1]/ (cm[0,1]+cm[1,1])\nprint(\"Specificity is\", specificity)\nprint(roc_auc_score(test_labels, dt.predict(test_data)))\ndt.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom matplotlib import pyplot as plt\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dt, \n                   filled=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf= RandomForestClassifier()\n\n        # Fit the classifier\nrf.fit(train_data, train_labels)\n\nac = accuracy_score(test_labels,rf.predict(test_data))\n#90+-4%\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(test_labels,rf.predict(test_data))\nsns.heatmap(cm,annot=True,fmt=\"d\")\nsensitivity=cm[0,0]/ (cm[0,0]+ cm[1,0])\nprint(\"Sensitivity is\",sensitivity)\nspecificity=cm[1,1]/ (cm[0,1]+cm[1,1])\nprint(\"Specificity is\", specificity)\nprint(roc_auc_score(test_labels,rf.predict(test_data)))\nrf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb= XGBClassifier(max_depth=7)\nxgb.fit(train_data, train_labels)\nac = accuracy_score(test_labels,xgb.predict(test_data))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(test_labels,xgb.predict(test_data))\nsns.heatmap(cm,annot=True,fmt=\"d\")\nsensitivity=cm[0,0]/ (cm[0,0]+ cm[1,0])\nprint(\"Sensitivity is\",sensitivity)\nspecificity=cm[1,1]/ (cm[0,1]+cm[1,1])\nprint(\"Specificity is\", specificity)\nprint(roc_auc_score(test_labels, xgb.predict(test_data)))\nxgb.get_params()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}